enc_fnn_sizes=[512]
enc_fnn_act='LeakyReLU'
enc_fnn_drop=[0.25]
enc_rnn_sizes=[256, 256, 256]
enc_rnn_bi=[True,True,True]
enc_rnn_skip=[2,2,1]
enc_rnn_drop=[0.0,0.0,0,25]
enc_rnn_cfgs={"type":"lstm", "bi":True}
downsampling=[False, True, True],
dec_emb_size=256
dec_emb_drop=0.6#en:0.8 ja:0.25
dec_emb_tied_weight=True,
# tying weight from char/word embedding with softmax layer
dec_rnn_sizes=512
dec_rnn_drop =0.7#en:0.8 ja:0.25
dec_rnn_cfgs={"type":"lstm"}
dec_cfg={"type":"standard_decoder"},
att_cfg={"type":"mlp"},
lr=0.001
EOS,SOS=2,1
maxlen={'word':30,'char':100,'p_word':30,'p_char':100}
